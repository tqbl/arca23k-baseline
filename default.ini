[Default]

# Identifier for the experiment
experiment_id = default

# Path to the parent directory containing the program output
work_dir = _workspace

# Path to the directory containing the ARCA23K dataset
arca23k_dir = _datasets/ARCA23K

# Path to the directory containing the FSD50K dataset
fsd50k_dir = _datasets/FSD50K

# Number of background workers for loading data
#
# Set to 0 to disable the use of background workers.
n_workers = 16

# Whether to use CUDA when available
cuda = true


[Extraction]

# Target sample rate
#
# Leave blank to use the original sample rate.
sample_rate = 32000

# Length of each block in seconds
#
# Audio clips are segmented into blocks of a fixed length.
# Leave blank to use the clip length as the block length.
block_length =

# Length of each hop in seconds
#
# This parameter determines the amount of overlap between adjacent
# blocks when segmenting the audio clips. If the hop length is equal to
# the block length, there is no overlap.
#
# Leave blank to use the block length as the hop length.
hop_length =

# Specification for extracting feature vectors
#
# Supported methods are 'mel' and 'spectrogram'.
features = {'method': 'mel', 'n_fft': 1024, 'hop_length': 512, 'n_mels': 64, 'top_db': 96}

# Whether to cache the extracted features into system memory
#
# This may significantly reduce training times, especially when
# resampling is enabled. The cost is an increase in memory usage and,
# for the _first_ epoch, an increase in training time.
cache_features = True


[Training]

# Neural network architecture
#
# Choices: vgg9a, vgg11a
model = vgg11a

# The fraction of the training set to use
frac = 1

# Path to checkpoint file containing model weights
#
# Leave blank for random initialization of weights.
weights_path =

# Parameters for adding label noise
#
# Format: {'method': 'uniform'|'class', 'rate': NUM}
# Leave blank or set to {} to disable.
label_noise =

# Seed used for random number generation
#
# Leave blank to skip setting a seed.
seed =

# Number of examples in a mini-batch
batch_size = 64

# Number of epochs to train the network 
n_epochs = 50

# Initial learning rate
lr = 0.0005

# Parameters for scheduling learning rate
lr_scheduler = {'method': 'step', 'step_size': 2, 'gamma': 0.9}

# Enables the use of multiple block lengths and batch sizes
#
# If this is enabled, the block length and batch size parameters defined
# above are ignored.
#
# Leave blank or set to {} to disable.
partition = {'block_lengths': [5, 10, 15], 'batch_sizes': [64, 32, 16]}

# Whether to overwrite any previously-saved checkpoints
#
# Setting this to false means that training will be resumed.
overwrite = False


[Prediction]

# Number of examples in a mini-batch
batch_size = 64

# Path to checkpoint file containing model weights
#
# Leave blank to use checkpoints corresponding to epochs instead.
weights_path =

# Enables the use of multiple block/hop lengths and batch sizes
partition = {'block_lengths': [5, 10, 15], 'batch_sizes': [64, 32, 16]}

# Name of output file
#
# Leave blank to use the default file name (see source code).
output_name =

# Seed used for random number generation
seed =

# Whether to remove unused checkpoint files
clean = False
